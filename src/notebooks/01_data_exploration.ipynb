{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0902ace",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from nba_api.stats.endpoints import leaguegamefinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a map of team abbreviations to full names\n",
    "\n",
    "team_map = { 'ATL': 'Atlanta Hawks', 'BOS': 'Boston Celtics', 'BKN': 'Brooklyn Nets',\n",
    "             'CHI': 'Chicago Bulls', 'CHA': 'Charlotte Hornets', 'CLE': 'Cleveland Cavaliers',\n",
    "             'DAL': 'Dallas Mavericks', 'DEN': 'Denver Nuggets', 'DET': 'Detroit Pistons',\n",
    "             'GSW': 'Golden State Warriors', 'HOU': 'Houston Rockets', 'IND': 'Indiana Pacers',\n",
    "             'LAC': 'Los Angeles Clippers', 'LAL': 'Los Angeles Lakers', 'MEM': 'Memphis Grizzlies',\n",
    "             'MIA': 'Miami Heat', 'MIL': 'Milwaukee Bucks', 'MIN': 'Minnesota Timberwolves',\n",
    "             'NOP': 'New Orleans Pelicans', 'NYK': 'New York Knicks', 'OKC': 'Oklahoma City Thunder',\n",
    "             'ORL': 'Orlando Magic', 'PHI': 'Philadelphia 76ers', 'PHX': 'Phoenix Suns',\n",
    "             'POR': 'Portland Trail Blazers', 'SAC': 'Sacramento Kings', 'SAS': 'San Antonio Spurs',\n",
    "             'TOR': 'Toronto Raptors', 'UTA': 'Utah Jazz',  'WAS': 'Washington Wizards' }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc1dbc",
   "metadata": {},
   "source": [
    "# Cleanup and Formatting of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the raw data and tweaking it a bit\n",
    "df = pd.read_csv(Path(\"../../data/raw/nba_2008-2025_RAW.csv\"))\n",
    "df = df.drop([\"moneyline_home\", \"moneyline_away\", \"h2_spread\", \"h2_total\"], axis=1, errors='ignore')\n",
    "df = df.dropna(how='any')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to save a df for each season\n",
    "\n",
    "seasons = df['season'].unique()\n",
    "for season in seasons:\n",
    "    season_df = df[df['season'] == season]\n",
    "    season_df.columns = season_df.columns.str.lower()\n",
    "    season_df.to_csv(Path(f\"../../data/interim/nba_{season}_CLEANED.csv\"), index=False)\n",
    "\n",
    "# as well as a cleaned version of the full dataset\n",
    "df.to_csv(Path(\"../../data/interim/nba_ALL_SEASONS_CLEANED.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c94cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api_data(seasons):\n",
    "    \"\"\"\n",
    "    Pulling data from the nba_api for each season from 2008 to 2025 and saving it as a csv.\n",
    "    2008 is the first season with spread and total data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seasons : list\n",
    "        List of season years (e.g., [2008, 2009, ..., 2025])\n",
    "    add_possessions : bool\n",
    "        Whether to fetch and add possession data (default: True)\n",
    "    api_delay : float\n",
    "        Delay between API calls in seconds (default: 0.6)\n",
    "    \"\"\"\n",
    "    for season in seasons:\n",
    "        filename = Path(f\"../../data/raw/nba_api_{season}_RAW.csv\")\n",
    "        \n",
    "        if not filename.exists():\n",
    "            print(f\"\\nProcessing season {season}...\")\n",
    "            \n",
    "            # Create season tag\n",
    "            if season % 100 < 10:\n",
    "                season_tag = f'{season-1}-0{season % 100}'\n",
    "            else: \n",
    "                season_tag = f'{season-1}-{season % 100}'\n",
    "            \n",
    "            # Fetch basic game data\n",
    "            print(f\"  Fetching game data for {season_tag}...\")\n",
    "            season_api = leaguegamefinder.LeagueGameFinder(\n",
    "                season_nullable=season_tag, \n",
    "                league_id_nullable='00'\n",
    "            )\n",
    "            df = season_api.get_data_frames()[0]        \n",
    "            df.columns = df.columns.str.lower()\n",
    "            \n",
    "            \n",
    "            # Save to CSV\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"  Saved to {filename}\")\n",
    "        else:\n",
    "            # File exists - check if possessions column needs to be added\n",
    "            print(f\"\\nSeason {season} file already exists...\")\n",
    "\n",
    "def merge_api_dfs(seasons):\n",
    "    api_dfs = []\n",
    "    for season in seasons:\n",
    "        api_df = pd.read_csv(Path(f\"../../data/raw/nba_api_{season}_RAW.csv\"))\n",
    "        api_dfs.append(api_df)\n",
    "\n",
    "    full_api_df = pd.concat(api_dfs, ignore_index=True)\n",
    "    full_api_df.to_csv(Path(\"../../data/raw/nba_ALL_SEASONS_API_RAW.csv\"), index=False)\n",
    "\n",
    "if Path(\"../../data/raw/nba_ALL_SEASONS_API_RAW.csv\").exists():\n",
    "    print(\"API data already exists. Skipping data pull.\")\n",
    "elif all(Path(f\"../../data/raw/nba_api_{season}_RAW.csv\").exists() for season in seasons):\n",
    "    print(\"All individual season API data files exist. Combining into one file.\")\n",
    "    merge_api_dfs(seasons)\n",
    "else:\n",
    "    load_api_data(list(range(2008, 2026)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ba900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_API_data(df):\n",
    "    \"\"\"\n",
    "    Cleans up discrepancies between data pulled from .csv and nba_api.\n",
    "    In general, we match to the format of the .csv data.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # remove null entries\n",
    "    df = df.dropna(how='any')\n",
    "    \n",
    "    # need to change their abbreviations\n",
    "    df['team_abbreviation'] = df['team_abbreviation'].replace({'NJN': 'BKN', 'NOH': 'NOP', 'SEA': 'OKC', 'CHO': 'CHA'})\n",
    "    # lets make the abreviations uppercase for consistency (just in case lowercase ones show up)\n",
    "    df['team_abbreviation'] = df['team_abbreviation'].str.upper()\n",
    "\n",
    "    # make team names match value from team_map\n",
    "    df['team_name'] = df['team_abbreviation'].map(team_map)\n",
    "\n",
    "    # drop any rows that don't have a valid team abbreviation\n",
    "    df = df.dropna(subset=['team_abbreviation', 'team_name'])\n",
    "\n",
    "    # change some column names for consistency with the other dataset\n",
    "    df = df.rename(columns={'season_id': 'season', 'PTS': 'points', 'game_date': 'date'})\n",
    "\n",
    "    # filtering to only actual games between two teams (no all-star/preseason games)\n",
    "    df = df[df['team_id'].astype(str).str.contains('127')] # the 127 is a common value found for actual teams, not All-star/preseason games\n",
    "    df = df[df['game_id'].duplicated(keep=False)]   # since each line corresponds to a single team, we remove all of the entries that don't have a matching team\n",
    "                                                                # this is mostly cleanup from all games that have a team whose ID doesn't contain '127'                                                    \n",
    "\n",
    "    df = df.sort_values(by='date') # sorting for consistency with CSV data\n",
    "\n",
    "    df = df.drop(columns=['team_id', 'team_name', 'min', 'plus_minus'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b634097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the API data a bit\n",
    "for season in seasons:\n",
    "    api_df = pd.read_csv(Path(f\"../../data/raw/nba_api_{season}_RAW.csv\")) \n",
    "\n",
    "    api_df = clean_API_data(api_df)    \n",
    "\n",
    "    print(api_df.head())\n",
    "    api_df.to_csv(Path(f\"../../data/interim/nba_api_{season}_CLEANED.csv\"), index=False)\n",
    "\n",
    "full_api_df = pd.read_csv(Path(\"../../data/raw/nba_ALL_SEASONS_API_RAW.csv\"))\n",
    "full_api_df = clean_API_data(full_api_df)\n",
    "full_api_df.to_csv(Path(\"../../data/interim/nba_ALL_SEASONS_API_CLEANED.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b07434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some formatting for csv data\n",
    "merged_dfs = []\n",
    "for season in seasons:\n",
    "    csv_df = pd.read_csv(Path(f\"../../data/interim/nba_{season}_CLEANED.csv\"))\n",
    "\n",
    "    # not much to do aside from changing the case of the team abbreviations\n",
    "    csv_df['away'] = csv_df['away'].str.upper()\n",
    "    csv_df['home'] = csv_df['home'].str.upper()\n",
    "\n",
    "    csv_df['away'] = csv_df['away'].replace({'NO': 'NOP', 'SA': 'SAS', 'NY': 'NYK', 'GS': 'GSW', 'UTAH': 'UTA', \"WSH\": \"WAS\"})\n",
    "    csv_df['home'] = csv_df['home'].replace({'NO': 'NOP', 'SA': 'SAS', 'NY': 'NYK', 'GS': 'GSW', 'UTAH': 'UTA', \"WSH\": \"WAS\"})\n",
    "\n",
    "    csv_df.to_csv(Path(f\"../../data/interim/nba_{season}_CLEANED.csv\"), index=False)\n",
    "    merged_dfs.append(csv_df)\n",
    "\n",
    "full_merged_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "full_merged_df.to_csv(Path(\"../../data/interim/nba_ALL_SEASONS_CLEANED.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce671533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding the csv data to have a row for each team in each game, mirroring the format of the API data\n",
    "full_csv_df = pd.read_csv(Path(\"../../data/interim/nba_ALL_SEASONS_CLEANED.csv\"))\n",
    "\n",
    "# Create a new DataFrame with duplicated entries for home_team = 0 and home_team = 1\n",
    "df_home = full_csv_df.copy()\n",
    "df_home['home_team'] = 1\n",
    "\n",
    "df_away = full_csv_df.copy()\n",
    "df_away['home_team'] = 0\n",
    "\n",
    "full_csv_df_expanded = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "full_csv_df_expanded['team_abbreviation'] = full_csv_df_expanded.apply(lambda row: row['home'] if row['home_team'] == 1 else row['away'], axis=1)\n",
    "full_csv_df_expanded['points'] = full_csv_df_expanded.apply(lambda row: row['score_home'] if row['home_team'] == 1 else row['score_away'], axis=1)\n",
    "full_csv_df_expanded = full_csv_df_expanded.sort_values(by=['date', 'home', 'team_abbreviation'])\n",
    "\n",
    "full_csv_df_expanded = full_csv_df_expanded.drop(columns=['home', 'away', 'score_home', 'score_away']).sort_values(by=['date', 'home_team', 'team_abbreviation'])\n",
    "\n",
    "full_csv_df_expanded.to_csv(Path(\"../../data/interim/nba_ALL_SEASONS_CLEANED_EXPANDED.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448979ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cleanup and expansion of the API data\n",
    "full_api_df = pd.read_csv(Path(\"../../data/interim/nba_ALL_SEASONS_API_CLEANED.csv\"))\n",
    "\n",
    "full_api_df['home_team'] = full_api_df['matchup'].str.contains('vs').astype(int)\n",
    "\n",
    "full_api_df = full_api_df.drop(columns=['matchup'])\n",
    "\n",
    "full_api_df.to_csv(Path(\"../../data/interim/nba_ALL_SEASONS_API_CLEANED_EXPANDED.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a81f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_games(api_df, csv_df):\n",
    "    \"\"\"\n",
    "    Matches games between api_df and csv_df based on 'date' and 'team_abbreviation'.\n",
    "    Returns a merged DataFrame with suffixes to distinguish columns from each source.\n",
    "    \"\"\"\n",
    "    merged = pd.merge(\n",
    "        api_df,\n",
    "        csv_df,\n",
    "        left_on=['date', 'team_abbreviation'],\n",
    "        right_on=['date', 'team_abbreviation'],\n",
    "        suffixes=('_api', '_csv'),\n",
    "        how='inner'\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "# Example usage:\n",
    "matched_df = match_games(full_api_df, full_csv_df_expanded)\n",
    "matched_df = matched_df.rename(columns={'home_team_api': 'home_team'})\n",
    "matched_df = matched_df.drop(columns=['season_api', 'season_csv', 'home_team_csv', 'points'])\n",
    "matched_df = matched_df.sort_values(by=['game_id'])\n",
    "matched_df.to_csv(Path(\"../../data/processed/nba_ALL_SEASONS_MATCHED.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7de52",
   "metadata": {},
   "source": [
    "# Data Exploration Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de712268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(Path(\"../../data/processed/nba_ALL_SEASONS_MATCHED.csv\"))\n",
    "\n",
    "# Distribution of total points scored by home and away teams\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "df[df['home_team'] == 1]['pts'].hist(bins=30, alpha=0.7, color='blue')\n",
    "plt.title('Home Team Score Distribution')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df[df['home_team'] == 0]['pts'].hist(bins=30, alpha=0.7, color='orange')\n",
    "plt.title('Away Team Score Distribution')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Spread distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['spread'].hist(bins=int(df['spread'].max()), color='green', alpha=0.7)\n",
    "plt.axvline(df['spread'].mean(), color='red', linestyle='dashed', linewidth=2, label=f'Mean: {df[\"spread\"].mean():.2f}')\n",
    "plt.legend()\n",
    "plt.title('Spread Distribution')\n",
    "plt.xlabel('Spread')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# corr = df[['pts','fgm','fga','fg_pct','fg3m','fg3a','fg3_pct','ftm','fta','ft_pct',\n",
    "#             'oreb','dreb','reb','ast','stl','blk','tov','pf','q1_away','q2_away','q3_away',\n",
    "#             'q4_away','ot_away','q1_home','q2_home','q3_home','q4_home','ot_home','id_spread','id_total']].corr()\n",
    "# print(df.info())\n",
    "# plt.matshow(corr, fignum=1, cmap='inferno')\n",
    "# plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "# plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "# for (i, col1) in enumerate(corr.columns):\n",
    "#     for (j, col2) in enumerate(corr.columns):\n",
    "#         if corr.iloc[i, j] > 0.25 and i != j:\n",
    "#             plt.text(j, i, f\"{corr.iloc[i, j]:.2f}\", va='center', ha='center', color='black', fontsize=6)\n",
    "# plt.colorbar()\n",
    "# plt.title('Correlation Matrix', pad=20)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
